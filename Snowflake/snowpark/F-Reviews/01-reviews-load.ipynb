{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews (load)\n",
    "\n",
    "Your business team is keen to understand how SnowBearAir is perceived by customers.  Our customers are our lifeblood, so understanding what areas of trouble or discontent exist is crucial to ensuring SnowBearAir remains at the top of industry polling, and customer satisfaction surveys.\n",
    "\n",
    "![](../assets/gateclerk_bear.png)\n",
    "\n",
    "Luckily, some airline reviews have been made available, [freely via CC0, on the web](https://www.kaggle.com/efehandanisman/skytrax-airline-reviews)\n",
    "\n",
    "Your job in this lab is to:\n",
    "\n",
    "- [ ] Upload the CSV file to a Snowflake internal stage\n",
    "- [ ] Create a StructType / UserSchema for reading the CSV file\n",
    "- [ ] Build a DataFrame that can read the data from the file\n",
    "- [ ] Load the entire file to the ALL_REVIEWS table\n",
    "\n",
    "![](../assets/reviews_load.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "import com.snowflake.snowpark.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set connection properties built in de_snowpark/A-Dataframes/01-Sessions.ipynb\n",
    "val pwd = sys.env.get(\"PWD\").fold(\"\")(_.toString)\n",
    "val filename = s\"$pwd/de_snowpark/connect.properties\"\n",
    "\n",
    "val session = Session.builder.configFile(s\"$filename\").create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put reviews.csv File\n",
    "\n",
    "In the following we use the file convenience method of Session to return a FileOperation object, which has access to `PUT`/`GET` data from Snowflake internal stages.  Notice that as part of the call `file.put()` we are also able to set options.  In our case below, we're instructing the `PUT` command to upload this file, without automatically GZIPing it by setting the AUTO_COMPRESS to FALSE.  See [PUT command](https://docs.snowflake.com/en/sql-reference/sql/put.html) for the full list of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.file.put(\"./reviews.csv.gz\", \"@~\", Map(\"OVERWRITE\"->\"TRUE\", \"AUTO_COMPRESS\" -> \"FALSE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fas fa-question fa-2x\"></i>\n",
    "    <b>Question:</b> What type of stage was <mark>reviews.csv.gz</mark> uploaded into?\n",
    "</div>\n",
    "\n",
    "\n",
    "### Progress: Check\n",
    "\n",
    "- [X] Upload the CSV file to a Snowflake internal stage\n",
    "- [ ] Create a StructType / UserSchema for reading the CSV file\n",
    "- [ ] Build a DataFrame that can read the data from the file\n",
    "- [ ] Load the entire file to the ALL_REVIEWS table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Schema that Maps CSV to SQL\n",
    "\n",
    "CSV files aren't strongly typed; they are a string of characters that can be converted to native SQL datatypes.  The name, position and datatype all need to be described to Snowflake for use in a DataFrame.  Here we are creating a Schema that indicates the position, column name, and Snowflake datatype of these columns. Keep in mind that this Snowpark concept of a Schema differs from a Database Schema in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val userSchema = StructType(Seq(\n",
    "    StructField(\"AIRLINE\", StringType)\n",
    "    , StructField(\"OVERALL\", ShortType)\n",
    "    , StructField(\"AUTHOR\", StringType)\n",
    "    , StructField(\"REVIEW_DATE\", StringType)\n",
    "    , StructField(\"CUSTOMER_REVIEW\", StringType)\n",
    "    , StructField(\"AIRCRAFT\", StringType)\n",
    "    , StructField(\"TRAVELLER_TYPE\", StringType)\n",
    "    , StructField(\"CABIN\", StringType)\n",
    "    , StructField(\"ROUTE\", StringType)\n",
    "    , StructField(\"DATE_FLOWN\", StringType)\n",
    "    , StructField(\"SEAT_COMFORT\", StringType)\n",
    "    , StructField(\"CABIN_SERVICE\", StringType)\n",
    "    , StructField(\"FOOD_BEV\", StringType)\n",
    "    , StructField(\"ENTERTAINMENT\", StringType)\n",
    "    , StructField(\"GROUND_SERVICE\", StringType)\n",
    "    , StructField(\"VALUE_FOR_MONEY\", StringType)\n",
    "    , StructField(\"RECOMMENDED\", StringType)    \n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress: Check\n",
    "\n",
    "- [X] Upload the CSV file to a Snowflake internal stage\n",
    "- [X] Create a StructType / UserSchema for reading the CSV file\n",
    "- [ ] Build a DataFrame that can read the data from the file\n",
    "- [ ] Load the entire file to the ALL_REVIEWS table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a DataFrame to Read the File Data\n",
    "\n",
    "The Session object has a `read` method that can be used to access data in a file in a Snowflake stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val reviewsDF = session\n",
    "    .read\n",
    "        .option(\"field_optionally_enclosed_by\", \"'\\\"'\")\n",
    "        .option(\"skip_header\", 1)\n",
    "    .schema(userSchema)       // We pass in the schema we created above                      \n",
    "    .csv(\"@~/reviews.csv.gz\") // Read the reviews file from the STAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fas fa-question fa-2x\"></i>\n",
    "    <b>Question:</b> Note the use of .option listed above (<mark>field_optionally_enclosed_by</mark> and <mark>skip_header</mark>)... What are these options exactly, and where are they documented in Snowflake?  Which SQL command will Snowpark build that will use these option definitions?\n",
    "</div>\n",
    "\n",
    "### Progress: Check\n",
    "\n",
    "- [X] Upload the CSV file to a Snowflake internal stage\n",
    "- [X] Create a StructType / UserSchema for reading the CSV file\n",
    "- [X] Build a DataFrame that can read the data from the file\n",
    "- [ ] Load the entire file to the ALL_REVIEWS table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `RAW.ALL_REVIEWS`\n",
    "\n",
    "Using the DataFrame above, we can now access and see values in the reviews.csv.gz file.  Let's load a table with the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsDF.write.saveAsTable(\"raw.ALL_REVIEWS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<i class=\"fas fa-question fa-2x\"></i>\n",
    "    <b>Question:</b> What command was executed above to perform the loading of the table?  An UPDATE/COPY/INSERT?\n",
    "</div>\n",
    "\n",
    "### Progress: Check\n",
    "\n",
    "- [X] Upload the CSV file to a Snowflake internal stage\n",
    "- [X] Create a StructType / UserSchema for reading the CSV file\n",
    "- [X] Build a DataFrame that can read the data from the file\n",
    "- [X] Load the entire file to the ALL_REVIEWS table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Try It\n",
    "\n",
    "You have two choices to review and play around with the data that was just loaded.\n",
    "\n",
    "Either:\n",
    "\n",
    "1. Head over to https://app.snowflake.com/ and login to the class Snowflake account using your animal name and password. Create a worksheet, and run the following commands.\n",
    "\n",
    "```sql\n",
    "// use [login]_db; -- use your default DB\n",
    "use schema raw;\n",
    "select * from raw.all_reviews limit 100;\n",
    "```\n",
    "\n",
    "2. Create a DataFrame object below and display a subset of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// hint, start with .table\n",
    "// then use .show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snowpark (Scala 2.12)",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
