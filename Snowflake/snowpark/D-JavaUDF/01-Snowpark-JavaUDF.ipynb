{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JavaUDF from Snowpark\n",
    "\n",
    "Snowflake can run your Java/Scala code as part of your overall data processing inside the Snowflake data processing engine.  \n",
    "\n",
    "![](../assets/java_udf_overview.gif)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Language Support:</b> Note that Snowflake's initial support for extensible user defined functions starts with Java/Scala, but will be not be limited to JVM languages in the future (such as Python)</div>\n",
    "\n",
    "This extensibility feature allows you to deploy and leverage your own internal programming expertise, along with the extensive set of Java libraries that already exist, to accomplish a myriad of data processing capabilities.\n",
    "\n",
    "In this section, we'll leverage *Snowpark* to collect and deploy our Scala code for building custom Java code that will run inside Snowflake.\n",
    "\n",
    "In this case, we'll be as simple as possible, and simply replace a `String` to understand the mechanics of where and how this code is deployed inside Snowflake.  We will also show locally running code to complement the server running code. In short, you can choose the location you want your code to run based on your specific requirements and needs with *Snowpark*.\n",
    "\n",
    "At the end of this lab, you will have run similar code in FOUR different ways:\n",
    "\n",
    "- [ ] Temporarily for a query, unnamed \n",
    "- [ ] Temporarily for a session, named\n",
    "- [ ] Permanently for Snowpark and SQL, named\n",
    "- [ ] Locally, leveraging Snowflake DataFrame `collections`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import Snowpark into our Scala notebook\n",
    "import com.snowflake.snowpark._\n",
    "import com.snowflake.snowpark.functions._\n",
    "import com.snowflake.snowpark.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set connection properties built in de_snowpark/A-Dataframes/01-Sessions.ipynb\n",
    "val pwd = sys.env.get(\"PWD\").fold(\"\")(_.toString)\n",
    "val filename = s\"$pwd/de_snowpark/connect.properties\"\n",
    "\n",
    "val session = Session.builder.configFile(s\"$filename\").create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Import existing implict objects (such as existing UDFs) for use by Snowpark\n",
    "import session.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a Snowflake internal stage that will be used by our Java UDFs\n",
    "session.sql(\"create stage if not exists raw.JAVA_UDF_STAGE\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our UDF class in Scala\n",
    "\n",
    "Rather than reinventing the wheel, we will create a single wheel (`class`) we'll use four different ways (`methods` used in UDFs differently) below.  We will use each of these methods to get a sense for how Snowpark can run our Java/Scala code as part of an entire pipeline using [DataFrames](https://docs.snowflake.com/en/developer-guide/snowpark/working-with-dataframes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class parrotClass() extends Serializable {\n",
    "    // Temporary (unnamed) says Hello\n",
    "    def sayHello = (s: String) => {\n",
    "        s\"Hello there $s\"\n",
    "    }\n",
    "    // Temporary (named) says Howdy\n",
    "    def sayHowdy = (s: String) => {\n",
    "        s\"Howdy $s\"\n",
    "    }\n",
    "    // Permanent (named) says Goodbye\n",
    "    def sayGoodbye = (s: String) => {\n",
    "        s\"Goodbye $s\"\n",
    "    }\n",
    "    // Local says Welcome to my town\n",
    "    def localSays = (s: String) => {\n",
    "        s\"Welcome to my town $s\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Test locally... just say Hello here in Scala to test our class\n",
    "\n",
    "// (nothing to do with Snowpark/Snowflake)\n",
    "new parrotClass().sayHello(\"Joe\")\n",
    "new parrotClass().sayHello(\"Cynthia\")\n",
    "new parrotClass().sayHowdy(\"Bobby\")\n",
    "new parrotClass().sayHowdy(\"Barbie\")\n",
    "new parrotClass().sayGoodbye(\"Xer\")\n",
    "new parrotClass().sayGoodbye(\"Ravi\")\n",
    "new parrotClass().localSays(\"David\")\n",
    "new parrotClass().localSays(\"Uday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sayHello: Create a Temporary UDF (unnamed)\n",
    "\n",
    "Let's start with a UDF that Snowpark will deploy as `temporary` and `unnamed`.  These temporary, unnamed UDFs typically are used only for a single query.  They can contain simple Scala code to do some more complex logic.  You can not reference this UDF without re-uploading the code in subsequent sessions; it's a one time use UDF.\n",
    "\n",
    "![](../assets/java_udf_temp_unnamed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Create a temporary unnamed UDF in Snowflake\n",
    "val sayHelloUDF = udf((new parrotClass()).sayHello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fas fa-question-circle fa-2x\"></i>\n",
    "<b>Queries</b>: What queries were executed on your behalf by Snowpark when you created a new UDF using the <mark>sayHello</mark> method?   Did you notice a .jar was uploaded on your behalf?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a temporary function, with some long machine-readable name (such as `tempUDF_939266449(arg1 STRING)`) that can be used in a Snowpark DataFrame.  Don't worry, we don't need to know what the temporary UDF name is. Once it is ready to go in Snowflake, we can just use the UDF object (`sayHelloUDF`) in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session\n",
    "    .sql(\"select 'Mohit' as name\")         // String Literal DataFrame\n",
    "    .withColumn(\n",
    "      \"fromjava\", sayHelloUDF(col(\"NAME\")) // Run through UDF in Snowflake!\n",
    "    ) \n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have successfully run your UDF code, uploaded and registered as a temporary unnamed Snowpark function, you will see the output of the Java code, run on Snowflake as follows:\n",
    "\n",
    "```\n",
    "------------------------------\n",
    "|\"NAME\"  |\"FROMJAVA\"         |\n",
    "------------------------------\n",
    "|Mohit   |Hello there Mohit  | <--- `Hello there Mohit` was the output of our Java code run on Snowflake!\n",
    "------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress: Check\n",
    "\n",
    "- [X] Temporarily for a query, unnamed \n",
    "- [ ] Temporarily for a session, named\n",
    "- [ ] Permanently for Snowpark and SQL, named\n",
    "- [ ] Locally, leveraging Snowflake DataFrame `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sayHowdy: create a Temporary UDF (named)\n",
    "\n",
    "More typical, and available for use in subsequent SQL, Snowpark can collect and deploy a temporary, named function.  This function is still temporary. It can be accessed via SQL, or instead of the created UDF object, it can also be invoked via the `callUDF` column expression function.\n",
    "\n",
    "![](../assets/java_udf_temp_named.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Register temporary function RAW.SAY_HOWDY()\n",
    "session.udf.registerTemporary(\"raw.SAY_HOWDY\", new parrotClass().sayHowdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fas fa-search fa-2x\"></i>\n",
    "<b>CREATE TEMPORARY</b>: The output of this looks similar to before... What was the function name that was created?  Was it machine-generated output? Was it what you expected to see?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session\n",
    "    .sql(\"select 'Alfred' as name\")          // Holy smokes, batman!\n",
    "    .withColumn(\"HOWDYNAME\"\n",
    "        , callUDF(\"raw.SAY_HOWDY\", col(\"name\")) \n",
    "    )\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fas fa-surprise fa-2x fa-border\"></i>\n",
    "    <b>Pop Quiz:</b> Will <mark>SAY_HOWDY()</mark> be available in SQL if you log in to Snowsight and run a query?  Why or why not? Let's try it below.\n",
    "</div>\n",
    "\n",
    "Head over to [https://app.snowflake.com/](https://app.snowflake.com/) and login to the class Snowflake account using your animal name and password. Create a worksheet, and run the following same SQL:\n",
    "\n",
    "```sql\n",
    "// use [login]_db; -- use your default DB\n",
    "use schema raw;\n",
    "\n",
    "SELECT  *  FROM ( \n",
    "  SELECT \n",
    "    \"NAME\"\n",
    "    , raw.SAY_HOWDY(\"NAME\") AS \"HOWDYNAME\" \n",
    "  FROM \n",
    "    (select 'Alfred' as name)\n",
    "  ) \n",
    "LIMIT 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress: Check\n",
    "\n",
    "- [X] Temporarily for a query, unnamed \n",
    "- [X] Temporarily for a session, named\n",
    "- [ ] Permanently for Snowpark and SQL, named\n",
    "- [ ] Locally, leveraging Snowflake DataFrame `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sayGoodbye: Create a Permanent UDF (named)\n",
    "\n",
    "Snowpark can also collect and push a Scala UDF and register it so that it's available in this Snowpark session, subsequent Snowpark sessions, and SQL.  The `registerPermanent` method creates a JavaUDF that is made available similar to any other scalar UDF in Snowflake (be it Java, JavaScript or SQL).\n",
    "\n",
    "![](../assets/java_udf_perm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Create a Permanent UDF in Snowflake\n",
    "session.udf.registerPermanent(\"raw.SAY_GOODBYE\", new parrotClass().sayGoodbye, \"raw.JAVA_UDF_STAGE\")\n",
    "// Since this UDF will stick around for a while, we need a permanent place to\n",
    "// hold the .jars and code for it.  JAVA_UDF_STAGE is a named stage that \n",
    "// we will use to hold this jar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session\n",
    "    .sql(\"select 'Henrietta' as name\")\n",
    "    .withColumn(\"GOODBYE_PERM\", callUDF(\"raw.SAY_GOODBYE\", col(\"name\"))) \n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head over to [https://app.snowflake.com/](https://app.snowflake.com/) and login to the class Snowflake account using your animal name and password. Create a worksheet, and run the following same SQL:\n",
    "\n",
    "```sql\n",
    "// use [login]_db; -- use your default DB\n",
    "use schema raw;\n",
    "\n",
    "SELECT  *  FROM ( \n",
    "  SELECT \n",
    "    \"NAME\"\n",
    "    , raw.SAY_GOODBYE(\"NAME\") AS \"GOODBYE_PERM\"\n",
    "  FROM (select 'Henrietta' as name)\n",
    "  )\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "It runs, right?  That's because `SAY_GOODBYE` has been created as a regular UDF and is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress: Check\n",
    "\n",
    "- [X] Temporarily for a query, unnamed \n",
    "- [X] Temporarily for a session, named\n",
    "- [X] Permanently for Snowpark and SQL, named\n",
    "- [ ] Locally, leveraging Snowflake DataFrame `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### localSays: Run local (here in notebook)\n",
    "\n",
    "With Snowpark, you have the choice on where you want to execute your Java.  If, for instance, you want to create your own programs and access remote network locations (for sending email/SMS/HTTPS) you can pair Snowflake DataFrames with your own local code.  The following isn't a UDF at all; it's actually showing that you can make use of DataFrames and interact with classes, **and** provide your own control programs.  \n",
    "\n",
    "![](../assets/java_udf_local.png)\n",
    "\n",
    "The [Scala world is your oyster](https://docs.scala-lang.org/overviews/collections/trait-traversable.html); you can `.fold`, `.map`, `.foreach` to your heart's content and pair that with your existing code!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session\n",
    "    .sql(\"select 'Rick' name union select 'Farnaz'\")\n",
    "    .collect()\n",
    "    // Above this line is running in Snowflake in the cloud\n",
    "    // Below this line is running locally here in the notebook\n",
    "    .foreach(row => {\n",
    "        println(new parrotClass().localSays(row.getString(0)))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress: Check\n",
    "\n",
    "- [X] Temporarily for a query, unnamed \n",
    "- [X] Temporarily for a session, named\n",
    "- [X] Permanently for Snowpark and SQL, named\n",
    "- [X] Locally, leveraging Snowflake DataFrame `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Optionally Cleanup your objects\n",
    "session.sql(\"drop function raw.say_howdy(varchar)\").collect\n",
    "session.sql(\"drop function raw.say_goodbye(varchar)\").collect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snowpark (Scala 2.12)",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
